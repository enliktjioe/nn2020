{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 2\n",
    "\n",
    "The homework consists of two parts: theoretical part (5 pts) and coding part (25 pts).\n",
    " - All theoretical questions must be answered in your own words, do not copy-paste text from the internet. Points can be deducted for terrible formatting or incomprehensible English.\n",
    " - Code must be commented. If you use code you found online, you have to add the link to the source you used. There is no penalty for using outside sources as long as you convince us you understand the code.\n",
    "\n",
    "*Once completed zip the entire directory containing this exercise and upload it to https://courses.cs.ut.ee/2020/nn/spring/Main/Practices.*\n",
    "\n",
    "\n",
    "For background reading see http://cs231n.github.io/linear-classify/ and http://cs231n.github.io/optimization-1/."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you did this homework together with course mates, please write here their names (answers still have to be your own!)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Name(s):** *fill this in if applicable*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Lecture Materials (5 pts)\n",
    "\n",
    "These theoretical questions are about the material covered in the lecture about \"Feed-forward Neural networks\"\n",
    "\n",
    "### Feed-forward Neural Networks\n",
    "\n",
    "**Task 1.1: Logic gates (1pt)**\n",
    "In the lecture you have seen how to construct `AND`, `OR` and `NOT` logic gates using the McCulloch-Pitts model of a neuron. Remember that:\n",
    "```\n",
    " if sum(w.*x)<0, output is 0\n",
    " if sum(w.*x)>=0, output is 1 (notice that 0 is included)\n",
    "```\n",
    " Your task is to construct one more logical operation using MP neurons - the `NAND` gate. Your response should contain an image (that you should draw yourself and not copy paste from the internet) similar to the images below:\n",
    "\n",
    "<img src=\"./gates.png\" width=\"250\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your Answer**: *fill this in.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 1.2: Perceptron (2pts)**\n",
    "In the lecture you learned about the perceptron algorithm that dates back to the 1950-60ies. Using a perceptron would it be possible to learn a model that perfectly separates the two classes in each of the following two datasets:\n",
    "\n",
    "**Dataset A:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Height | Weights | Label |\n",
    "|--------|---------|-------|\n",
    "| 25cm   | 5kg     | dog   |\n",
    "| 22cm   | 3kg     | dog   |\n",
    "| 24cm   | 3kg     | cat   |\n",
    "| 23cm   | 5kg     | cat   |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Dataset B:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Height | Weights | Label |\n",
    "|--------|---------|-------|\n",
    "| 25cm   | 5kg     | dog   |\n",
    "| 24cm   | 6kg     | dog   |\n",
    "| 23cm   | 4kg     | cat   |\n",
    "| 24cm   | 5kg     | cat   |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each dataset your response should state if perceptron can learn to separate the classes and if not, why. Feel free to add illustrations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your Answer**: *fill this in.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 1.3: Feed-forward neural network (2pts)**\n",
    "Consider two fully-connected feed-forward neural networks that take as input 32x32 images with 3 color values per pixel. This input is flattened before being fed to the network (put into one long 1D vector). The networks' output consists of 10 probabilities corresponding to different object classes (thus performing object recognition). \n",
    "\n",
    "Network A has one hidden layer of size 1024. Network B has three hidden layers of size 512. \n",
    "\n",
    "Answer the following questions:\n",
    " - What is the length of the input vector?\n",
    " - What are the dimensions of the weight matrices of these networks? (you need to give 2 matrices for network A and 4 for network B) \n",
    " - Which network has more parameters? Explain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your Answer**: *fill this in.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Softmax exercise (25 pts)\n",
    "\n",
    "In this exercise you will:\n",
    "\n",
    "- implement a fully-vectorized **loss function** for the Softmax classifier\n",
    "- implement the fully-vectorized expression for its **analytic gradient**\n",
    "- **check your implementation** with numerical gradient\n",
    "- **optimize** the loss function with **SGD**\n",
    "- use a validation set to **tune the learning rate and regularization** strength\n",
    "- **visualize** the final learned weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Theory\n",
    "\n",
    "We have a linear classifier (you could also call it 1-layer neural network). Input to the network is a vector $\\mathbf{x}$ of $D$ features, output of the network is vector of $C$ class probablities $\\mathbf{p}$. The target class $c$ is coded as one-hot vector $\\mathbf{y}$ (meaning it has 1 at index $c$ and zeroes everywhere else). Weights of the network are represented by $N \\times C$ matrix $\\mathbf{W}$.\n",
    "\n",
    "$$\n",
    "\\mathbf{x} = \\begin{pmatrix} x_1 & x_2 & ..& x_D \\end{pmatrix}\n",
    "\\qquad\n",
    "\\mathbf{p} = \\begin{pmatrix} p_1\\\\ p_2\\\\ ..\\\\ p_C \\end{pmatrix}\n",
    "\\qquad\n",
    "\\mathbf{y} = \\begin{pmatrix} y_1\\\\ y_2\\\\ ..\\\\ y_C \\end{pmatrix}\n",
    "\\qquad y_i = \n",
    "\\begin{cases}\n",
    "    1, &\\textrm{if}\\ \\ i=c\\\\\n",
    "    0, &otherwise\n",
    "\\end{cases}\n",
    "\\qquad\n",
    "\\mathbf{W} = \\begin{pmatrix} w_{11}&w_{12}&..&w_{1C}\\\\w_{21}&w_{22}&..&w_{2C}\\\\..&..&..&..\\\\w_{D1}&w_{D2}&..&w_{DC} \\end{pmatrix}\n",
    "$$\n",
    "\n",
    "Notice we are missing **biases**, but without a loss of generalization we can add another feature to the inputs which is always $1$ and this turns weights of this feature into biases. \n",
    "\n",
    "To train the network we use cross-entropy loss function and perform gradient descent with respect to weight matrix $\\mathbf{W}$. Following represents step-by-step forward pass of the network, where $L$ is the loss function:\n",
    "\n",
    "\n",
    "\\begin{align*}\n",
    "z_j = \\sum_{i=1}^D x_i W_{ij},\\qquad\\qquad \\textrm{which can be achieved via} \\qquad\\qquad \\mathbf{z} &= \\mathbf{x}W, \n",
    "\\\\\n",
    "\\qquad p_i = \\frac{e^{z_i}}{\\sum_{j=1}^C e^{z_j}},\\qquad\\qquad\\qquad \\textrm{  or in short} \\qquad\\qquad\\qquad\\qquad \\mathbf{p} &= softmax(\\mathbf{z}),\n",
    "\\end{align*}\n",
    "\n",
    "\\begin{align*}\n",
    "L &= -\\sum_{i=1}^C y_i \\log p_i = -\\log p_c \\qquad\\qquad\\qquad \\textrm{ (only c-th element matters)}\n",
    "\\end{align*}\n",
    "\n",
    "\n",
    "To perform gradient descent we need the gradient of loss function with respect to the weights $\\frac{\\partial L}{\\partial W_{ij}}$. This can be expressed with chain rule:\n",
    "\n",
    "$$\n",
    "%\\frac{\\partial L}{\\partial \\mathbf{W}} = \\frac{\\partial L}{\\partial \\mathbf{p}} \\cdot \\frac{\\partial \\mathbf{p}}{\\partial \\mathbf{z}} \\cdot \\frac{\\partial \\mathbf{z}}{\\partial \\mathbf{W}}\n",
    "%\\\\\n",
    "\\frac{\\partial L}{\\partial W_{ij}} = \\sum_{k=1}^{C} \\sum_{l=1}^{C} \\frac{\\partial L}{\\partial p_l} \\frac{\\partial p_l}{\\partial z_k} \\frac{\\partial z_k}{\\partial W_{ij}}\n",
    "$$\n",
    "\n",
    "Sum over $l$ and $k$ comes from the fact that $W_{ij}$ affects loss $L$ through multiple pathways through different $p_l$ and $z_k$.  \n",
    "It is best imagined as a [computational graph](http://colah.github.io/posts/2015-08-Backprop/).\n",
    "\n",
    "To compute the full gradient we need to produce three partial derivatives:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{align*}\n",
    "\\frac{\\partial L}{\\partial p_l} &= \\ldots\n",
    "\\\\\n",
    "\\frac{\\partial p_l}{\\partial z_k} &= \\ldots\n",
    "\\\\\n",
    "\\frac{\\partial z_k}{\\partial W_{ij}} &= \\ldots\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 2.1 (5pts):** \n",
    "Derive the above gradients and fill the empty space above. You need to remember some classical calculus rules:\n",
    "$$\n",
    "(e^x)' = e^x\n",
    "\\qquad\n",
    "(\\log x)' = \\frac{1}{x}\n",
    "\\qquad\n",
    "(xy)' = x'y + x'y\n",
    "\\qquad\n",
    "\\left(\\frac{x}{y}\\right)' = \\frac{x'y - y'x}{y^2}\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&nbsp; \n",
    "\n",
    "&nbsp; \n",
    "\n",
    "For you to be able to complete the coding tasks, we give you the correct final formulas, obtained when putting the above three partial derivatives together:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\frac{\\partial L}{\\partial z_k} &= \\sum_{l=1}^C \\frac{\\partial L}{\\partial p_l} \\frac{\\partial p_l}{\\partial z_k}= -\\frac{y_k}{p_k} p_k (1-p_k) + \\sum_{l\\neq k}^C \\frac{y_l}{p_l} p_l p_k = -y_k + y_k p_k + \\sum_{l\\neq k}^C y_l p_k  = -y_k + p_k \\sum_{l=1}^C y_l= p_k-y_k\n",
    "\\\\\n",
    "\\frac{\\partial L}{\\partial W_{ij}} &= \\sum_{k=1}^C \\frac{\\partial L}{\\partial z_k} \\frac{\\partial z_k}{\\partial W_{ij}} = (p_j - y_j) x_i\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "This computes the gradient value in one data point. However, for increased stability, it is beneficial to learn from average gradient over multiple points. The collection of N samples is called a **minibatch**. We note the loss in k-th data point by $L_k$ and the average loss with $\\textbf{L}$.\n",
    "\n",
    "\\begin{align*}\n",
    "\\textbf{L}=\\frac{1}{N}\\sum_{k=1}^N L_k \n",
    "\\end{align*}\n",
    "\n",
    "\\begin{align*}\n",
    "\\frac{\\partial \\textbf{L}}{\\partial W_{ij}} &= \\frac{1}{N} \\sum_{l=1}^N \\frac{\\partial L_k}{\\partial W_{ij}}\n",
    "\\end{align*}\n",
    "\n",
    "\n",
    "It is convenient to work with average loss and average gradients, not the sum, because then the magnitude of the value does not depend on batch-size N.\n",
    "\n",
    "----\n",
    "\n",
    "To perform a gradient descent you need to subtract gradient from the weights (because we are minimizing the loss function):\n",
    "$$\n",
    "W_{ij}^{t+1} = W_{ij}^t - \\alpha \\frac{\\partial \\textbf{L}}{\\partial W_{ij}}\n",
    "$$\n",
    "\n",
    "\n",
    "Here $\\alpha$ is a learning rate that must be tuned manually.\n",
    "\n",
    "Using the learning rule and the formulas for calculating gradients for each element of W, you should be able to complete the first coding task below (marked with task 2.2). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "from data_utils import load_CIFAR10\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "# for auto-reloading extenrnal modules\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CIFAR-10 Data Loading and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the raw CIFAR-10 data.\n",
    "cifar10_dir = '../datasets/cifar-10-batches-py'\n",
    "X_train, y_train, X_test, y_test = load_CIFAR10(cifar10_dir)\n",
    "\n",
    "# As a sanity check, we print out the size of the training and test data.\n",
    "print('Training data shape: ', X_train.shape)\n",
    "print('Training labels shape: ', y_train.shape)\n",
    "print('Test data shape: ', X_test.shape)\n",
    "print('Test labels shape: ', y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize some examples from the dataset.\n",
    "# We show a few examples of training images from each class.\n",
    "classes = ['plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "num_classes = len(classes)\n",
    "samples_per_class = 7\n",
    "for y, cls in enumerate(classes):\n",
    "    idxs = np.flatnonzero(y_train == y)\n",
    "    idxs = np.random.choice(idxs, samples_per_class, replace=False)\n",
    "    for i, idx in enumerate(idxs):\n",
    "        plt_idx = i * num_classes + y + 1\n",
    "        plt.subplot(samples_per_class, num_classes, plt_idx)\n",
    "        plt.imshow(X_train[idx].astype('uint8'))\n",
    "        plt.axis('off')\n",
    "        if i == 0:\n",
    "            plt.title(cls)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into train, val, and test sets. In addition we will\n",
    "# create a small development set as a subset of the training data;\n",
    "# we can use this for development so our code runs faster.\n",
    "num_training = 49000\n",
    "num_validation = 1000\n",
    "num_test = 1000\n",
    "num_dev = 500\n",
    "\n",
    "# Our validation set will be num_validation points from the original\n",
    "# training set.\n",
    "mask = range(num_training, num_training + num_validation)\n",
    "X_val = X_train[mask]\n",
    "y_val = y_train[mask]\n",
    "\n",
    "# Our training set will be the first num_train points from the original\n",
    "# training set.\n",
    "mask = range(num_training)\n",
    "X_train = X_train[mask]\n",
    "y_train = y_train[mask]\n",
    "\n",
    "# We will also make a development set, which is a small subset of\n",
    "# the training set.\n",
    "mask = np.random.choice(num_training, num_dev, replace=False)\n",
    "X_dev = X_train[mask]\n",
    "y_dev = y_train[mask]\n",
    "\n",
    "# We use the first num_test points of the original test set as our\n",
    "# test set.\n",
    "mask = range(num_test)\n",
    "X_test = X_test[mask]\n",
    "y_test = y_test[mask]\n",
    "\n",
    "print('Train data shape: ', X_train.shape)\n",
    "print('Train labels shape: ', y_train.shape)\n",
    "print('Validation data shape: ', X_val.shape)\n",
    "print('Validation labels shape: ', y_val.shape)\n",
    "print('Test data shape: ', X_test.shape)\n",
    "print('Test labels shape: ', y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing: reshape the image data into rows\n",
    "X_train = np.reshape(X_train, (X_train.shape[0], -1))\n",
    "X_val = np.reshape(X_val, (X_val.shape[0], -1))\n",
    "X_test = np.reshape(X_test, (X_test.shape[0], -1))\n",
    "X_dev = np.reshape(X_dev, (X_dev.shape[0], -1))\n",
    "\n",
    "# As a sanity check, print out the shapes of the data\n",
    "print('Training data shape: ', X_train.shape)\n",
    "print('Validation data shape: ', X_val.shape)\n",
    "print('Test data shape: ', X_test.shape)\n",
    "print('dev data shape: ', X_dev.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing: subtract the mean image\n",
    "# first: compute the image mean based on the training data\n",
    "mean_image = np.mean(X_train, axis=0)\n",
    "print(mean_image[:10]) # print a few of the elements\n",
    "plt.figure(figsize=(4,4))\n",
    "plt.imshow(mean_image.reshape((32,32,3)).astype('uint8')) # visualize the mean image\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# second: subtract the mean image from train and test data\n",
    "X_train -= mean_image\n",
    "X_val -= mean_image\n",
    "X_test -= mean_image\n",
    "X_dev -= mean_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# third: append the bias dimension of ones (i.e. bias trick) so that our model\n",
    "# only has to worry about optimizing a single weight matrix W.\n",
    "X_train = np.hstack([X_train, np.ones((X_train.shape[0], 1))])\n",
    "X_val = np.hstack([X_val, np.ones((X_val.shape[0], 1))])\n",
    "X_test = np.hstack([X_test, np.ones((X_test.shape[0], 1))])\n",
    "X_dev = np.hstack([X_dev, np.ones((X_dev.shape[0], 1))])\n",
    "\n",
    "print(X_train.shape, X_val.shape, X_test.shape, X_dev.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Softmax Classifier\n",
    "\n",
    "Your code for this section will all be written inside `softmax.py`.\n",
    "\n",
    "**Task 2.2 (2pts):** First implement the naive softmax loss function with nested loops. Open the file `softmax.py` and implement the `softmax_loss_naive` function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: implement code in softmax.py\n",
    "from softmax import softmax_loss_naive\n",
    "import time\n",
    "\n",
    "# Generate a random softmax weight matrix and use it to compute the loss.\n",
    "W = np.random.randn(3073, 10) * 0.0001\n",
    "loss, grad = softmax_loss_naive(W, X_dev, y_dev, 0.0)\n",
    "\n",
    "# As a rough sanity check, our loss should be something close to -log(0.1).\n",
    "print('loss: %f' % loss)\n",
    "print('sanity check: %f' % (-np.log(0.1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 2.3 (1pt):** Why do we expect our loss to be close to $-\\log(0.1)$? Explain briefly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your answer:** *Fill this in*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 2.4 (2pts):** Complete the implementation of `softmax_loss_naive` and implement a (naive) version of the gradient that uses nested loops."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: implement code in softmax.py\n",
    "loss, grad = softmax_loss_naive(W, X_dev, y_dev, 0.0)\n",
    "\n",
    "# Use numeric gradient checking as a debugging tool.\n",
    "# The numeric gradient should be close to the analytic gradient\n",
    "# (relative error should be in the magnitude of 1e-8).\n",
    "print(\"Without regularization:\")\n",
    "from gradient_check import grad_check_sparse\n",
    "f = lambda w: softmax_loss_naive(w, X_dev, y_dev, 0.0)[0]\n",
    "grad_numerical = grad_check_sparse(f, W, grad, 10)\n",
    "\n",
    "# Do another gradient check with regularization.\n",
    "print(\"With regularization:\")\n",
    "loss, grad = softmax_loss_naive(W, X_dev, y_dev, 5e1)\n",
    "f = lambda w: softmax_loss_naive(w, X_dev, y_dev, 5e1)[0]\n",
    "grad_numerical = grad_check_sparse(f, W, grad, 10)\n",
    "\n",
    "# Hint: the gardients your code finds and the gradients computed numerically should more-or-less agree\n",
    "# you can expect on average 10^-8 error. A 10^-6 might also happen. Anything bigger is suspicious."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorization of the network and learning\n",
    "\n",
    "By now you have (hopefully) implemented how to do the froward pass and apply the gradient calculation element by element to the parameters in the weight matrix. However, this code performed the gradient comutation for one data point at a time, with a surrounding for-loop collecting values over N data points and later averaging them. \n",
    "\n",
    "As demonstrated on distance calculations in the previous practice, calculating values using for loops is time-consuming. We now look for a way to get rid of for-loops and compute values in a vecotrized way for all N data points in one go.\n",
    "\n",
    "We now have as inputs:\n",
    "\n",
    "**X:** Matrix of shape (N, D) containing a minibatch of data.  \n",
    "**W:** Matrix of shape (D, C) containing weights.  \n",
    "The correct labels can be cosidered as one-hot encoded, forming also a matrix:  \n",
    "**Y:** Matrix of shape (N,C) containing the labels\n",
    "\n",
    "---------\n",
    "First of all, we look at the **forward pass.** \n",
    "  \n",
    "Above, we showed that given one row vector of inputs (shape (1,D)), the weighted summing corresponds to the matrix multiplication:\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\mathbf{z} &= \\mathbf{x}W \\qquad \n",
    "\\end{align*}\n",
    "$$\n",
    "  \n",
    "This also holds for a matrix of inputs where each line is a data point (N,D):\n",
    "\n",
    "\\begin{align*}\n",
    "\\mathbf{Z} &= \\mathbf{X}W,\n",
    "\\end{align*}\n",
    "\n",
    "which can be shown on whiteboard. The result is a NxC matrix of **Z**, **where again each line of activations ccorresponds to a data point.** The softmax operation is also applied line-by-line, so each line of P corresponds to one input. Finally, the cross-entropy loss calculation is done line-by-line resulting in a loss value per data point - shape (N,1).\n",
    "\n",
    "So we have  \n",
    "**Z:** Matrix of shape (N, C) containing activation vectors.  \n",
    "**P:** Matrix of shape (N, C) containing output probabilities.  \n",
    "**Losses:** Vector of shape (N, 1) containing losses.\n",
    "\n",
    "We wish to average over the losses as in the naive implementation.\n",
    "  \n",
    "\n",
    "  \n",
    "  \n",
    "----\n",
    "  \n",
    "**Backward pass:** Secondly, we need to calculate the gradients with respect to the weights.\n",
    "\n",
    "In our above notation for one sample, where L was a real number and x was a vector, we had:\n",
    "\\begin{align*}\n",
    "\\frac{\\partial L}{\\partial W_{ij}} &= (p_j - y_j) x_i\n",
    "\\end{align*}\n",
    "\n",
    "In here, X is (N,D) matrix. We showed above that each line of Z, P and Y correspond to one data point and depend on only one line of X. So, we can express average gradient over samples as\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\frac{\\partial \\textbf{L}}{\\partial W_{ij}} &= \\frac{1}{N} \\sum_{k=1}^N x_{ki}(p_{kj}-y_{kj})\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "This sum can be achieved via the following matrix multiplication:    \n",
    "\\begin{align*}\n",
    "\\frac{1}{N}\\textbf{X}^T \\ (P \\ - \\ Y) &=\\frac{1}{N} \\sum_{k=1}^N x_{ki}(p_{kj}-y_{kj}) \n",
    "\\end{align*}\n",
    "\n",
    "The correspondence can be showed on whiteboard in class, if needed.  \n",
    "\n",
    "The matrix multiplication formulas for finding **Z**, and $\\frac{\\partial \\textbf{L}}{\\partial W_{ij}}$, combined with some broadcasting, are sufficient to implement the forward and backward passes without for loops."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 2.5 (6pts):** Now that we have a naive implementation of the softmax loss function and its gradient, implement a vectorized version in `softmax_loss_vectorized`. The two versions should compute the same results, but the vectorized version should be much faster (around 10x)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: implement code in softmax.py\n",
    "tic = time.time()\n",
    "loss_naive, grad_naive = softmax_loss_naive(W, X_dev, y_dev, 0.000005)\n",
    "toc = time.time()\n",
    "print('naive loss: %e computed in %fs' % (loss_naive, toc - tic))\n",
    "\n",
    "from softmax import softmax_loss_vectorized\n",
    "tic = time.time()\n",
    "loss_vectorized, grad_vectorized = softmax_loss_vectorized(W, X_dev, y_dev, 0.000005)\n",
    "toc = time.time()\n",
    "print('vectorized loss: %e computed in %fs' % (loss_vectorized, toc - tic))\n",
    "\n",
    "# We use the Frobenius norm to compare the two versions of the gradient.\n",
    "grad_difference = np.linalg.norm(grad_naive - grad_vectorized, ord='fro')\n",
    "print('Loss difference: %f' % np.abs(loss_naive - loss_vectorized))\n",
    "print('Gradient difference: %f' % grad_difference)\n",
    "\n",
    "#Hint, these two differences have to be 0.0000, otherwise you must have a bug. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stochastic Gradient Descent\n",
    "\n",
    "We now have vectorized and efficient expressions for the loss, the gradient and our gradient matches the numerical gradient. We are therefore ready to do SGD to minimize the loss.\n",
    "\n",
    "**Task 2.6 (2pts):** In the file `linear_classifier.py`, implement SGD in the function `LinearClassifier.train()` and then run it with the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: implement code in linear_classifier.py\n",
    "from linear_classifier import Softmax\n",
    "model = Softmax()\n",
    "tic = time.time()\n",
    "loss_hist = model.train(X_train, y_train, learning_rate=1e-7, reg=2.5e4,\n",
    "                      num_iters=1500, verbose=True)\n",
    "toc = time.time()\n",
    "print('That took %fs' % (toc - tic))\n",
    "\n",
    "#Hint: you should observe the loss going down, starting from hundreds and ending up near 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A useful debugging strategy is to plot the loss as a function of\n",
    "# iteration number:\n",
    "plt.plot(loss_hist)\n",
    "plt.xlabel('Iteration number')\n",
    "plt.ylabel('Loss value')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 2.7 (1pt):** Write the `Softmax.predict` function and evaluate the performance on both the training and validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: implement code in linear_classifier.py\n",
    "y_train_pred = model.predict(X_train)\n",
    "print('training accuracy: %f' % (np.mean(y_train == y_train_pred), ))\n",
    "y_val_pred = model.predict(X_val)\n",
    "print('validation accuracy: %f' % (np.mean(y_val == y_val_pred), ))\n",
    "\n",
    "#hint, if everything is correct, the results should be in range 0.3 to 0.35"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 2.8 (4pts):**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the validation set to tune hyperparameters (regularization strength and\n",
    "# learning rate). You should experiment with different ranges (orders of magnitude)\n",
    "# for the learning rates and regularization strengths.\n",
    "#\n",
    "# YOUR TASK IS to get a classification accuracy of over 0.35 on the validation set.\n",
    "from linear_classifier import Softmax\n",
    "results = {}\n",
    "best_val = -1\n",
    "best_softmax = None\n",
    "\n",
    "################################################################################\n",
    "# TODO:                                                                        #\n",
    "# Write code that chooses the best hyperparameters by tuning on the validation #\n",
    "# set. For each combination of hyperparameters, train a linear model on the    #\n",
    "# training set, compute its accuracy on the training and validation sets, and  #\n",
    "# store these numbers in the results dictionary. In addition, store the best   #\n",
    "# validation accuracy in best_val and the Softmax object that achieves this    #\n",
    "# accuracy in best_softmax.                                                    #\n",
    "# TODO: the best model must achieve at least 0.35 accuracy                     #\n",
    "# TODO: if not all parameters you tried are visible in the final submitted     #\n",
    "# version, you might add a comment about what else you tried earlier.          #\n",
    "#                                                                              #\n",
    "# Hint: You should use a small value for num_iters as you develop your hyper-  #\n",
    "# parameter search code so that the models don't take much time to train; once #\n",
    "# you are confident that your code works, you should rerun the code with a     #\n",
    "# larger value for num_iters.                                                  #\n",
    "# The same approach might be useful to first determine the coarse range of     #\n",
    "# useful values for both parameters (should it be near 0.1 or 0.0001?) and then#\n",
    "# perform a finer-grained search.                                              #\n",
    "################################################################################\n",
    "learning_rates = []  #fill this with LR you want to try, good LRs are quite small\n",
    "regularization_strengths = []  #the regs you want to try should be bigger\n",
    "pass\n",
    "################################################################################\n",
    "#                              END OF YOUR CODE                                #\n",
    "################################################################################\n",
    "    \n",
    "# Print out results.\n",
    "for lr, reg in sorted(results):\n",
    "    train_accuracy, val_accuracy = results[(lr, reg)]\n",
    "    print('lr %e reg %e train accuracy: %f val accuracy: %f' % (\n",
    "                lr, reg, train_accuracy, val_accuracy))\n",
    "    \n",
    "print('best validation accuracy achieved during cross-validation: %f' % best_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the cross-validation results\n",
    "import math\n",
    "x_scatter = [math.log10(x[0]) for x in results]\n",
    "y_scatter = [math.log10(x[1]) for x in results]\n",
    "\n",
    "# plot training accuracy\n",
    "marker_size = 100\n",
    "colors = [results[x][0] for x in results]\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.scatter(x_scatter, y_scatter, marker_size, c=colors)\n",
    "plt.colorbar()\n",
    "plt.xlabel('log learning rate')\n",
    "plt.ylabel('log regularization strength')\n",
    "plt.title('CIFAR-10 training accuracy')\n",
    "\n",
    "# plot validation accuracy\n",
    "colors = [results[x][1] for x in results] # default size of markers is 20\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.scatter(x_scatter, y_scatter, marker_size, c=colors)\n",
    "plt.colorbar()\n",
    "plt.xlabel('log learning rate')\n",
    "plt.ylabel('log regularization strength')\n",
    "plt.title('CIFAR-10 validation accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate on test set\n",
    "# Evaluate the best softmax on test set\n",
    "y_test_pred = best_softmax.predict(X_test)\n",
    "test_accuracy = np.mean(y_test == y_test_pred)\n",
    "print('softmax on raw pixels final test set accuracy: %f' % (test_accuracy, ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the learned weights for each class\n",
    "w = best_softmax.W[:-1,:] # strip out the bias\n",
    "w = w.reshape(32, 32, 3, 10)\n",
    "\n",
    "w_min, w_max = np.min(w), np.max(w)\n",
    "\n",
    "classes = ['plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "for i in range(10):\n",
    "    plt.subplot(2, 5, i + 1)\n",
    "    \n",
    "    # Rescale the weights to be between 0 and 255\n",
    "    wimg = 255.0 * (w[:, :, :, i].squeeze() - w_min) / (w_max - w_min)\n",
    "    plt.imshow(wimg.astype('uint8'))\n",
    "    plt.axis('off')\n",
    "    plt.title(classes[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 2.9 (2pts):** Describe what your visualized weights look like, and offer a brief explanation for why they look the way that they do."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**Your answer:** *fill this in*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
